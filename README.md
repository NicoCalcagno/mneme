# ğŸ§  Mneme

> *Dal greco Î¼Î½Î®Î¼Î· (mnÄ“mÄ“) - "memoria, ricordo"*

Mneme trasforma il tuo vault Obsidian in un cervello digitale interrogabile. Attraverso RAG (Retrieval-Augmented Generation), indicizza tutte le tue note in un vector store e ti permette di conversare naturalmente con la tua conoscenza personale.

Non piÃ¹ ricerca manuale tra centinaia di note: Mneme comprende il contesto, trova connessioni semantiche e recupera istantaneamente le informazioni rilevanti per rispondere alle tue domande.

---

## âœ¨ Features

- ğŸ—‚ï¸ **Ingestion automatica** - Sincronizza automaticamente le tue note da Obsidian
- ğŸ” **Semantic Search** - Ricerca basata sul significato, non solo su keyword
- ğŸ’¬ **Conversational Interface** - Chatta con le tue note in linguaggio naturale
- ğŸ§© **Obsidian-aware** - Supporto per wikilinks, backlinks, tags e metadata
- ğŸ”„ **Multi-LLM Support** - Passa da OpenAI ad Anthropic, Mistral o altri provider
- ğŸ“Š **Built-in Observability** - Monitoraggio performance con OpenTelemetry
- ğŸš€ **Production Ready** - Basato su Datapizza AI, framework italiano enterprise-grade
- ğŸ”’ **Privacy First** - PossibilitÃ  di eseguire tutto in locale

---

## ğŸ—ï¸ Architettura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Obsidian Vault                        â”‚
â”‚                  (markdown files)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Ingestion Pipeline  â”‚
          â”‚  - ObsidianParser    â”‚
          â”‚  - TextSplitter      â”‚
          â”‚  - Embeddings        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Vector Store       â”‚
          â”‚   (Qdrant/Chroma)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   RAG Agent          â”‚
          â”‚   (Datapizza AI)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    FastAPI           â”‚
          â”‚    REST API          â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisiti

- **Docker** (metodo consigliato) oppure
- **Python >= 3.10, < 3.13** + [uv](https://github.com/astral-sh/uv)
- Un vault Obsidian esistente
- API Key di un provider LLM (OpenAI, Anthropic, etc.)

---

### ğŸ³ Metodo 1: Docker (Consigliato)

Il metodo piÃ¹ semplice per avviare Mneme Ã¨ utilizzare Docker.

#### Setup rapido

```bash
# 1. Clona il repository
git clone https://github.com/tuousername/mneme.git
cd mneme

# 2. Configura le variabili d'ambiente
cp .env.example .env
nano .env  # Modifica con i tuoi parametri

# 3. Avvia con Docker
docker-compose up -d

# 4. Verifica che sia attivo
curl http://localhost:8000/api/v1/health
```

#### Comandi Docker utili

```bash
# Avvia il server
docker-compose up -d

# Visualizza i log
docker-compose logs -f

# Ferma il server
docker-compose down

# Rebuild dopo modifiche
docker-compose up -d --build

# Avvia con osservabilitÃ  (include Zipkin)
docker-compose --profile observability up -d
```

#### Oppure con Docker diretto (senza compose)

```bash
# Build dell'immagine
docker build -t mneme:latest .

# Avvio del container
docker run -d \
  --name mneme \
  -p 8000:8000 \
  --env-file .env \
  mneme:latest

# Visualizza i log
docker logs -f mneme

# Ferma il container
docker stop mneme && docker rm mneme
```

---

### ğŸ Metodo 2: Installazione locale con Python

Per sviluppo o se preferisci non usare Docker.

#### Installazione

```bash
# 1. Installa uv (se non giÃ  installato)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Clona il repository
git clone https://github.com/tuousername/mneme.git
cd mneme

# 3. Crea virtual environment e installa dipendenze
uv venv
source .venv/bin/activate  # Su Windows: .venv\Scripts\activate

# 4. Installa il progetto
uv pip install -e .

# Per sviluppo (include pytest, black, ruff, mypy)
uv pip install -e ".[dev]"

# Per embeddings locali (include sentence-transformers)
uv pip install -e ".[local]"

# Per installare tutto
uv pip install -e ".[all]"
```

#### Configurazione

```bash
# Copia il file di esempio
cp .env.example .env

# Modifica .env con i tuoi parametri
nano .env
```

#### Avvio del server

```bash
# Metodo 1: Usando il comando installato
mneme-serve

# Metodo 2: Usando Python direttamente
python -m api.main

# Con auto-reload per development
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

---

### âš™ï¸ Configurazione `.env`

Copia `.env.example` in `.env` e modifica i seguenti parametri:

```env
# =========================================================================
# OBSIDIAN VAULT
# =========================================================================
OBSIDIAN_VAULT_PATH=/path/to/your/obsidian/vault
OBSIDIAN_FILE_EXTENSIONS=.md,.markdown
OBSIDIAN_EXCLUDE_FOLDERS=.obsidian,.trash,templates

# =========================================================================
# LLM PROVIDER
# =========================================================================
LLM_PROVIDER=openai                    # openai, anthropic, google, mistral
LLM_MODEL=gpt-4o
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# =========================================================================
# API KEYS
# =========================================================================
OPENAI_API_KEY=your-openai-api-key-here
# ANTHROPIC_API_KEY=your-anthropic-key  # Se usi Anthropic

# =========================================================================
# EMBEDDINGS
# =========================================================================
EMBEDDING_PROVIDER=openai              # openai o local
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
EMBEDDING_BATCH_SIZE=100

# =========================================================================
# VECTOR STORE
# =========================================================================
VECTOR_STORE_TYPE=qdrant               # qdrant o chroma
VECTOR_STORE_COLLECTION=mneme_knowledge

# Qdrant Cloud (production)
QDRANT_URL=https://your-cluster.region.cloud.qdrant.io
QDRANT_API_KEY=your-qdrant-api-key

# Qdrant Locale (development)
# QDRANT_PATH=./data/qdrant
# Commenta QDRANT_URL e QDRANT_API_KEY se usi locale

# =========================================================================
# CHUNKING
# =========================================================================
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNKING_STRATEGY=recursive            # recursive, fixed, semantic

# =========================================================================
# RETRIEVAL
# =========================================================================
RETRIEVAL_TOP_K=5
RETRIEVAL_MIN_SCORE=0.7

# =========================================================================
# API SERVER
# =========================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true                        # Auto-reload in development
API_WORKERS=1
CORS_ORIGINS=*
API_PREFIX=/api/v1
ENABLE_DOCS=true

# =========================================================================
# OBSERVABILITY
# =========================================================================
ENABLE_TRACING=false
LOG_LEVEL=INFO
LOG_FILE=./data/logs/mneme.log

# =========================================================================
# DEVELOPMENT
# =========================================================================
ENVIRONMENT=development                # development, production, testing
DEBUG=false
```

---

### ğŸ§ª Verifica l'installazione

```bash
# 1. Controlla la health dell'API
curl http://localhost:8000/api/v1/health

# 2. Visualizza la documentazione interattiva
open http://localhost:8000/docs

# 3. Testa gli endpoint
curl http://localhost:8000/api/v1/ready
curl http://localhost:8000/api/v1/live
```

**Risposta health check di successo:**
```json
{
  "status": "healthy",
  "version": "0.1.0",
  "uptime_seconds": 123.45,
  "timestamp": "2025-10-21T20:30:00Z",
  "vector_store_connected": true,
  "llm_provider": "openai",
  "checks": {
    "vector_store": true
  }
}
```
---

## ğŸ“ Struttura del Progetto

```
mneme/
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md                     # Guida per Claude Code
â”œâ”€â”€ pyproject.toml                # Configurazione progetto e dipendenze
â”œâ”€â”€ docker-compose.yaml           # Docker Compose setup
â”œâ”€â”€ Dockerfile                    # Docker image build
â”œâ”€â”€ .env.example                  # Template configurazione
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ __init__.py                   # Root package init
â”‚
â”œâ”€â”€ api/                          # ğŸŒ REST API (FastAPI)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # FastAPI app entry point
â”‚   â”œâ”€â”€ dependencies.py           # Dependency injection
â”‚   â”œâ”€â”€ middleware.py             # Custom middleware (logging, etc)
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat.py               # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ health.py             # Health check endpoints
â”‚   â”‚   â””â”€â”€ ingestion.py          # Ingestion trigger endpoints
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ enums.py              # Enumerations
â”‚       â”œâ”€â”€ common.py             # Shared models
â”‚       â”œâ”€â”€ chat.py               # Chat request/response models
â”‚       â”œâ”€â”€ health.py             # Health check models
â”‚       â””â”€â”€ ingestion.py          # Ingestion models
â”‚
â”œâ”€â”€ config/                       # âš™ï¸  Configurazione
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py               # Pydantic Settings (env vars)
â”‚
â”œâ”€â”€ ingestion/                    # ğŸ“¥ Pipeline di ingestion
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parser.py                 # (TODO) ObsidianParser
â”‚   â”œâ”€â”€ chunker.py                # (TODO) Text splitting
â”‚   â”œâ”€â”€ embedder.py               # (TODO) Embedding generation
â”‚   â”œâ”€â”€ vectorstore.py            # (TODO) Vector DB operations
â”‚   â””â”€â”€ ingest.py                 # (TODO) CLI entry point
â”‚
â”œâ”€â”€ rag/                          # ğŸ¤– RAG Agent
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py                  # (TODO) Datapizza AI agent
â”‚   â”œâ”€â”€ retriever.py              # (TODO) Custom retrieval
â”‚   â””â”€â”€ prompts.py                # (TODO) System prompts
â”‚
â”œâ”€â”€ utils/                        # ğŸ› ï¸  Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py                 # (TODO) Logging utilities
â”‚
â”œâ”€â”€ scripts/                      # ğŸ“œ Scripts di utilitÃ 
â”‚   â””â”€â”€ test-qdrant.py            # Test connessione Qdrant
â”‚
â”œâ”€â”€ tests/                        # (TODO) Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â””â”€â”€ test_api.py
â”‚
â””â”€â”€ data/                         # ğŸ’¾ Gitignored
    â”œâ”€â”€ qdrant/                   # Vector DB storage (locale)
    â””â”€â”€ logs/                     # Application logs
```

**Nota sulla struttura:** I moduli marcati con `(TODO)` sono placeholder che verranno implementati nelle prossime fasi del progetto.

---

## ğŸ”§ API Endpoints

L'API REST Ã¨ disponibile su `http://localhost:8000` con documentazione interattiva su `/docs`.

### ğŸ¥ Health Check

#### `GET /api/v1/health`
Verifica lo stato di salute dell'applicazione e delle sue dipendenze.

```bash
curl http://localhost:8000/api/v1/health
```

**Risposta:**
```json
{
  "status": "healthy",
  "version": "0.1.0",
  "uptime_seconds": 123.45,
  "timestamp": "2025-10-21T20:30:00Z",
  "vector_store_connected": true,
  "llm_provider": "openai",
  "vector_store_documents": null,
  "checks": {
    "vector_store": true
  }
}
```

#### `GET /api/v1/ready`
Kubernetes-style readiness probe.

```bash
curl http://localhost:8000/api/v1/ready
```

#### `GET /api/v1/live`
Kubernetes-style liveness probe.

```bash
curl http://localhost:8000/api/v1/live
```

---

### ğŸ’¬ Chat (TODO - Da implementare)

#### `POST /api/v1/chat`
Invia un messaggio al chatbot e ricevi una risposta basata sulle tue note Obsidian.

```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Cosa ho scritto sul machine learning?",
    "conversation_id": "optional-session-id",
    "include_sources": true,
    "temperature": 0.7,
    "max_tokens": 2000
  }'
```

**Risposta prevista:**
```json
{
  "conversation_id": "conv_abc123",
  "message": "Nelle tue note sul machine learning...",
  "sources": [
    {
      "file_path": "notes/ml/intro.md",
      "chunk_id": "chunk_001",
      "score": 0.95,
      "content": "Il machine learning Ã¨...",
      "metadata": {
        "tags": ["ml", "ai"],
        "created_at": "2024-01-15"
      }
    }
  ],
  "processing_time_ms": 1234.56,
  "metadata": {
    "model": "gpt-4o",
    "temperature": 0.7
  }
}
```

#### `GET /api/v1/chat/conversations`
Lista tutte le conversazioni salvate.

```bash
curl http://localhost:8000/api/v1/chat/conversations
```

#### `DELETE /api/v1/chat/conversations/{conversation_id}`
Elimina una conversazione specifica.

```bash
curl -X DELETE http://localhost:8000/api/v1/chat/conversations/conv_abc123
```

---

### ğŸ“¥ Ingestion (TODO - Da implementare)

#### `POST /api/v1/ingest`
Avvia manualmente il processo di ingestion delle note Obsidian.

```bash
curl -X POST http://localhost:8000/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "vault_path": "/path/to/vault",
    "incremental": true,
    "force": false,
    "dry_run": false
  }'
```

**Risposta:**
```json
{
  "status": "in_progress",
  "message": "Ingestion started in background",
  "files_processed": null,
  "files_skipped": null,
  "total_chunks": null,
  "processing_time_s": null
}
```

#### `GET /api/v1/ingest/status`
Ottieni lo stato corrente del processo di ingestion.

```bash
curl http://localhost:8000/api/v1/ingest/status
```

---

### ğŸ“š Documentazione Interattiva

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

La documentazione interattiva permette di testare tutti gli endpoint direttamente dal browser.

---

## ğŸ¯ Roadmap

### âœ… Fase 1: Foundation (Completata)
- [x] Setup progetto con uv e pyproject.toml
- [x] Configurazione con Pydantic Settings
- [x] Struttura API REST con FastAPI
- [x] Health check endpoints
- [x] Docker e docker-compose setup
- [x] Middleware di logging con request tracking
- [x] Documentazione API interattiva (Swagger/ReDoc)

### ğŸš§ Fase 2: Ingestion Pipeline (In Corso)
- [ ] ObsidianParser per markdown + frontmatter
- [ ] Supporto wikilinks e backlinks
- [ ] Text chunking (recursive, fixed, semantic)
- [ ] Integrazione embeddings (OpenAI/local)
- [ ] Connessione Qdrant vector store
- [ ] CLI per ingestion (`mneme-ingest`)
- [ ] Ingestion incrementale con tracking

### ğŸ“‹ Fase 3: RAG Agent
- [ ] Setup Datapizza AI agent
- [ ] Custom retrieval logic
- [ ] System prompts per note Obsidian
- [ ] Implementazione endpoint `/chat`
- [ ] Gestione conversazioni e context
- [ ] Source citations nel response

### ğŸ¨ Fase 4: Features Avanzate
- [ ] Supporto multi-LLM (OpenAI, Anthropic, Mistral)
- [ ] Embeddings locali con sentence-transformers
- [ ] Support per immagini nelle note
- [ ] Graph-based retrieval
- [ ] Webhook per sync automatica
- [ ] Frontend web UI
- [ ] Multi-vault support

### ğŸš€ Fase 5: Production
- [ ] Test suite completa
- [ ] CI/CD pipeline
- [ ] Monitoring e metriche
- [ ] Rate limiting
- [ ] Caching
- [ ] Plugin Obsidian nativo

---

## ğŸ› ï¸ Tech Stack

- **[Datapizza AI](https://github.com/datapizza-labs/datapizza-ai)** - Framework RAG e Agent
- **FastAPI** - REST API framework
- **Qdrant** / **ChromaDB** - Vector database
- **OpenAI** / **Anthropic** - LLM providers
- **OpenTelemetry** - Observability
- **Pydantic** - Data validation

---

## ğŸ¤ Contributing

Contributi benvenuti! Apri una issue o una pull request.

1. Fork del progetto
2. Crea un branch (`git checkout -b feature/amazing-feature`)
3. Commit delle modifiche (`git commit -m 'Add amazing feature'`)
4. Push al branch (`git push origin feature/amazing-feature`)
5. Apri una Pull Request

---

## ğŸ“„ License

Questo progetto Ã¨ rilasciato sotto licenza MIT. Vedi il file [LICENSE](LICENSE) per i dettagli.

---

## ğŸ™ Credits

- Built with [Datapizza AI](https://datapizza.tech/en/ai-framework/) ğŸ•
- Ispirato dalla community Obsidian
- Nome dal greco antico *Mneme* (ÎœÎ½Î®Î¼Î·), dea della memoria

---

## ğŸ“§ Contatti

- **Issues**: [GitHub Issues](https://github.com/tuousername/mneme/issues)
- **Discussioni**: [GitHub Discussions](https://github.com/tuousername/mneme/discussions)

---

<p align="center">
  Made with ğŸ§  and â˜• 
</p>